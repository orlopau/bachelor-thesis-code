# Distributed Deep Learning

* supervised learning
* mostly gradient descent
* minibatch gradient descent commonly used
* entire pass = epoch

* single machine parallelism -> one machine multiple gpus/cpus
* multi machine parallelism -> multiple machine with multiple gpus/cpus

* concurrent sgd with mini batches
* batch size important parameter
* has upper bound, too high

(Demystifying Parallel and Distributed Deep Learning:
An In-depth Concurrency Analysis)